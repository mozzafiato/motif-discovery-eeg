{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import stumpy\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import Utils\n",
    "utils = Utils()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_cv(X, y, fs=True):\n",
    "    f1_train_list = []\n",
    "    f1_test_list = []\n",
    "    acc_train_list = []\n",
    "    acc_test_list = []\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        if fs:\n",
    "            features = run_feature_selection2(RFE, model)\n",
    "            X_selected = X[:, features]\n",
    "            selected_columns = training_set.columns[features]\n",
    "            print(sorted(selected_columns))\n",
    "        else:\n",
    "            X_selected = X\n",
    "        \n",
    "        results = cross_validate(model, X_selected, y, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "    \n",
    "        f1_train_list.append(results['train_f1'].mean())\n",
    "        acc_train_list.append(results['train_accuracy'].mean())\n",
    "        f1_test_list.append(results['test_f1'].mean())\n",
    "        acc_test_list.append(results['test_accuracy'].mean())\n",
    "        \n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"model\"] = models\n",
    "    result_df[\"F1_train\"] = f1_train_list\n",
    "    result_df[\"Acc_train\"] = acc_train_list\n",
    "    result_df[\"F1_val\"] = f1_test_list\n",
    "    result_df[\"Acc_val\"] = acc_test_list\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_gender(y_true, y_pred, gender_labels):\n",
    "    y_true = np.array(y_true)\n",
    "    gender_0_ind = np.where(np.array(gender_labels) == 0)\n",
    "    gender_1_ind = np.where(np.array(gender_labels) == 1)\n",
    "    #print(gender_labels)\n",
    "    #print(\"overall acc:\", accuracy(y_pred, y_true))\n",
    "    ##sns.heatmap(confusion_matrix(y_pred, y_true), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "    \n",
    "    gender_0_accuracy = accuracy(y_pred[gender_0_ind], y_true[gender_0_ind])\n",
    "    #print(\"Gender 0 accuracy:\", gender_0_accuracy)\n",
    "    #sns.heatmap(confusion_matrix(y_pred[gender_0_ind], y_true[gender_0_ind]), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "               \n",
    "    gender_1_accuracy = accuracy(y_pred[gender_1_ind], y_true[gender_1_ind])\n",
    "    #print(\"Gender 1 accuracy:\", gender_1_accuracy)\n",
    "    #sns.heatmap(confusion_matrix(y_pred[gender_1_ind], y_true[gender_1_ind]), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "    \n",
    "    return gender_0_accuracy, gender_1_accuracy\n",
    "    \n",
    "    \n",
    "#trained_model, f1_train, f1_test, acc_train, acc_test = fit_evaluate_model(models[0], X_train, X_test_final, y_train, y_test_final, print_evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model(model, X_train, X_test, y_train, y_test, train_gender, validation_gender, model_name=None, print_evaluation=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"Training set\")\n",
    "    y_pred = model.predict(X_train)\n",
    "    f1_train = f1_score(y_train, y_pred)\n",
    "    acc_train = accuracy(y_train, y_pred)\n",
    "    #print(\"F1:\", f1_train)\n",
    "    #print(\"Acc:\", acc_train)\n",
    "    \n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_train,y_pred))\n",
    "        cm = confusion_matrix(y_train,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "        \n",
    "    #print(len(y_train))\n",
    "    #print(len(train_gender))\n",
    "    gender_train_acc = evaluate_by_gender(y_train, y_pred, train_gender)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #print(\"Testing set\")\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "    acc_test = accuracy(y_test, y_pred)\n",
    "    #print(\"F1:\", f1_test)\n",
    "    #print(\"Acc:\", acc_test)\n",
    "\n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "        \n",
    "    gender_test_acc = evaluate_by_gender(y_test, y_pred, validation_gender)\n",
    "        \n",
    "    \n",
    "    return model, f1_train, f1_test, acc_train, acc_test, gender_train_acc, gender_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# obtained after hyperparameter tunning\n",
    "models = [\n",
    "    SVC(kernel='linear', C=0.05),  \n",
    "    LinearSVC(penalty=\"l1\", dual=False),\n",
    "    DecisionTreeClassifier(max_depth=3,criterion=\"gini\"),\n",
    "    RandomForestClassifier(n_estimators=5,max_depth=3,min_samples_leaf=2,min_samples_split=4, random_state=2), \n",
    "    LogisticRegression(C=0.7),\n",
    "    MLPClassifier(hidden_layer_sizes=(5,10,5))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def run_feature_selection2(selector_class, model, estimator=None):\n",
    "    f1s = []\n",
    "    f1s_train = []\n",
    "    best_chosen = []\n",
    "    f1_max = 0.0\n",
    "    f1_train_ = None\n",
    "    \n",
    "    for i in range(3, X.shape[1]):\n",
    "\n",
    "        if estimator == None:\n",
    "            selector = selector_class(model, n_features_to_select=i, step=1)\n",
    "        else:\n",
    "            selector = selector_class(estimator, n_features_to_select=i, step=1)\n",
    "        selector = selector.fit(X, y)\n",
    "        chosen = selector.get_support()\n",
    "        X_selected = X[:, chosen]\n",
    "        #print(X_train_selected.shape)\n",
    "\n",
    "        results = cross_validate(model, X_selected, y, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "        f1_train = results['train_f1'].mean()\n",
    "        f1 = results['test_f1'].mean()\n",
    "\n",
    "        #print(f1_train)\n",
    "        f1s_train.append(f1_train)\n",
    "        #print(f1)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "        if f1 > f1_max:\n",
    "            f1_max = f1\n",
    "            f1_train_ = f1_train\n",
    "            best_chosen = chosen\n",
    "            \n",
    "        if f1_train > 0.95 and f1_train - f1 > 0.4:\n",
    "            print(\"Overfitting..\")\n",
    "            break\n",
    "        \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 10\n",
    "    fig_size[1] = 5\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "    plt.plot(range(1,len(f1s)+1), f1s, label=\"Validation set\")\n",
    "    plt.plot(range(1,len(f1s)+1), f1s_train, label=\"Training set\")\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    print(\"--------------\")\n",
    "    indexes = list(range(X.shape[1]))\n",
    "    chosen_indexes = np.array(indexes)[best_chosen]\n",
    "    print(chosen_indexes)\n",
    "    print(len(chosen_indexes))\n",
    "    print(\"Training:\", f1_train_)\n",
    "    print(\"Testing:\", f1_max)\n",
    "    \n",
    "    return chosen_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets(band, path=\"./feature_matrices/\"):\n",
    "    training_set = pd.read_csv(path+\"motifs_{}_train.csv\".format(band))\n",
    "    validation_set = pd.read_csv(path+\"motifs_{}_val.csv\".format(band))\n",
    "    test_set = pd.read_csv(path+\"motifs_{}_test.csv\".format(band))\n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for band in [\"alpha\", \"beta\", \"theta\"]:\n",
    "\n",
    "    best_f1s = []\n",
    "\n",
    "    training_set, validation_set, testing_set = read_datasets(band, path=\"./feature_matrices/\")\n",
    "    target = \"label\"\n",
    "    y_train = training_set[target]\n",
    "    X_train = np.array(training_set.drop(target, axis=1))\n",
    "    y_val = validation_set[target]\n",
    "    X_val = np.array(validation_set.drop(target, axis=1))\n",
    "    X_test = np.array(testing_set.drop(target, axis=1))\n",
    "    y_test = testing_set[target]\n",
    "\n",
    "    X = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "    y = np.concatenate((y_train, y_val, y_test))\n",
    "\n",
    "    #result_df = run_evaluation_result(X_train, X_test, y_train, y_test)\n",
    "    #display(result_df)\n",
    "\n",
    "    print(\"CV:\")\n",
    "    result_df = run_evaluation_cv(X, y)\n",
    "    ind_max = result_df['F1_val'].idxmax()\n",
    "\n",
    "    display(result_df)\n",
    "    best_f1s.append(result_df.iloc[ind_max])\n",
    "    print(result_df.iloc[ind_max])\n",
    "\n",
    "\n",
    "    print(\"===============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
