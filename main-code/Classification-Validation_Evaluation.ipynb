{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import stumpy\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import Utils\n",
    "utils = Utils()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band=\"theta\"\n",
    "utils.read_data(band=band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences = utils.get_all_patient_signals(dataset=\"train\")\n",
    "labels = np.array(utils.get_labels(labeling=1), dataset=\"train\")\n",
    "gender_info = utis.get_genders(dataset=\"train\")\n",
    "\n",
    "test_sequences = utils.get_all_patient_signals(dataset=\"test\")\n",
    "test_labels = utils.get_labels(labeling=1, dataset=\"test\")\n",
    "test_final_gender = utils.get_genders(dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = []\n",
    "validation_sequences = []\n",
    "train_labels = []\n",
    "validation_labels = []\n",
    "train_gender = []\n",
    "validation_gender = []\n",
    "\n",
    "for i in [0, 1]:\n",
    "    print(\"Gender:\", i)\n",
    "    gender_ind = list(np.argwhere(gender_info == i).T[0])\n",
    "\n",
    "    all_sequences_ = [sequences for j, sequences in enumerate(all_sequences) if j in gender_ind]\n",
    "    print(len(all_sequences_))\n",
    "    labels_ = [label for j, label in enumerate(labels) if j in gender_ind]\n",
    "    labels_ = np.array(labels_)\n",
    "\n",
    "    split_ratio = 0.15\n",
    "    if i == 1:\n",
    "        split_ratio = 0.3\n",
    "        \n",
    "    train_sequences_, validation_sequences_, train_labels_, validation_labels_, train_ind, val_ind = train_test_split(all_sequences_, labels_, gender_ind, stratify=labels_, test_size=split_ratio, random_state=2)\n",
    "    print(\"--Train:\", len(train_sequences_))\n",
    "    print(\"patient indexes:\", train_ind)\n",
    "    print(\"----\", sum(train_labels_ == 0))\n",
    "    print(\"----\", sum(train_labels_ == 1))\n",
    "    print(\"--Val:\", len(validation_sequences_))\n",
    "    print(\"patient indexes:\", val_ind)\n",
    "    print(\"----\", sum(validation_labels_ == 0))\n",
    "    print(\"----\", sum(validation_labels_ == 1))\n",
    "    \n",
    "    train_sequences.append(train_sequences_)\n",
    "    validation_sequences.append(validation_sequences_)\n",
    "    train_labels.append(list(train_labels_))\n",
    "    validation_labels.append(list(validation_labels_))\n",
    "    train_gender.append([i]*len(train_sequences_))\n",
    "    validation_gender.append([i]*len(validation_sequences_))\n",
    "\n",
    "# flatten\n",
    "train_sequences = sum(train_sequences, [])\n",
    "validation_sequences = sum(validation_sequences, [])\n",
    "train_labels = np.array(sum(train_labels, []))\n",
    "validation_labels = np.array(sum(validation_labels, []))\n",
    "train_gender = sum(train_gender, [])\n",
    "validation_gender = sum(validation_gender, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=500\n",
    "df = pd.read_csv(\"motifs_{}_m{}.csv\".format(band, length))\n",
    "df.sort_values(\"diff_scores\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(group):\n",
    "    return group.sort_values(\"diff_scores\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_matrix(input_sequences, input_motifs, input_electrodes, input_indexes, input_ids):\n",
    "    X = []\n",
    "    # iterate over given patients\n",
    "    for j, sequences in enumerate(input_sequences):\n",
    "        #print(\"Patient\", j+1)\n",
    "        patient_motif_distances = []\n",
    "        # iterate over all discovered motifs\n",
    "        for i, motif in enumerate(input_motifs):\n",
    "            electrode = input_electrodes[i]\n",
    "            original_signal = train_sequences[input_ids[i]][electrode]\n",
    "            motif = original_signal[input_indexes[i]:input_indexes[i]+m]\n",
    "            #if type(motif) == str:\n",
    "            #    try:\n",
    "            #        motif = np.array(ast.literal_eval(' '.join(motif.split()).replace(\"[ \", \"[\").replace(\"\\n\", \"\").replace(\" \", \", \")), dtype=np.float64)\n",
    "            #    except:\n",
    "            #        print(\"error reading motif\")\n",
    "            #        print(motif)\n",
    "            #        print(' '.join(motif.split()).replace(\"[ \", \"[\").replace(\"\\n\", \"\").replace(\" \", \", \"))\n",
    "                        \n",
    "\n",
    "\n",
    "            signal = sequences[electrode]\n",
    "            matches = stumpy.match(motif, signal, max_distance=None, max_matches=3, normalize=True, p=2.0)\n",
    "            if len(matches) == 0:\n",
    "                patient_motif_distances.append(np.nan)\n",
    "            else:\n",
    "                patient_motif_distances.append(np.min(matches[:, 0]))\n",
    "        X.append(patient_motif_distances)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(grouped_df):\n",
    "    \n",
    "    final_motifs =  grouped_df[\"motifs\"].tolist()\n",
    "    final_electrodes = grouped_df[\"electrodes\"].tolist()\n",
    "    final_genders = grouped_df[\"genders\"].tolist()\n",
    "    final_labels = grouped_df[\"labels\"].tolist()\n",
    "    final_indexes = grouped_df[\"indexes\"].tolist()\n",
    "    final_ids = grouped_df[\"train_ind\"].tolist()\n",
    "    \n",
    "    motif_names = []\n",
    "    for i, _ in enumerate(final_motifs):\n",
    "        motif_names.append(\"motif_el_{}_class_{}_gender_{}_id_{}\".format(final_electrodes[i], final_labels[i], final_genders[i], i))\n",
    "    \n",
    "    X_train = generate_feature_matrix(train_sequences, final_motifs, final_electrodes, final_indexes, final_ids)\n",
    "    print(len(X_train))\n",
    "    print(len(X_train[0]))\n",
    "    df_train = pd.DataFrame(data = np.array(X_train), columns = motif_names)\n",
    "    df_train[\"label\"] = train_labels\n",
    "\n",
    "    X_validation_final = generate_feature_matrix(validation_sequences, final_motifs, final_electrodes, final_indexes, final_ids)\n",
    "    print(len(X_validation_final))\n",
    "    print(len(X_validation_final[0]))\n",
    "    df_validation = pd.DataFrame(data = np.array(X_validation_final), columns = motif_names)\n",
    "    df_validation[\"label\"] = validation_labels\n",
    "    \n",
    "    X_test_final = generate_feature_matrix(test_sequences, final_motifs, final_electrodes, final_indexes, final_ids)\n",
    "    print(len(X_test_final))\n",
    "    print(len(X_test_final[0]))\n",
    "    df_test = pd.DataFrame(data = np.array(X_test_final), columns = motif_names)\n",
    "    df_test[\"label\"] = test_labels\n",
    "\n",
    "    return df_train, df_validation, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_result(X_input_train, X_input_test,  y_input_train, y_input_test, train_input_gender, validation_input_gender):\n",
    "    f1_train_list = []\n",
    "    f1_test_list = []\n",
    "    acc_train_list = []\n",
    "    acc_test_list = []\n",
    "    gender_train_0_acc = []\n",
    "    gender_train_1_acc = []\n",
    "    gender_test_0_acc = []\n",
    "    gender_test_1_acc = []\n",
    "\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        trained_model, f1_train, f1_test, acc_train, acc_test, gender_train_acc, gender_test_acc = fit_evaluate_model(model, X_input_train, X_input_test, y_input_train, y_input_test, train_input_gender, validation_input_gender, print_evaluation=False)\n",
    "        f1_train_list.append(f1_train)\n",
    "        f1_test_list.append(f1_test)\n",
    "        acc_train_list.append(acc_train)\n",
    "        acc_test_list.append(acc_test)\n",
    "        gender_train_0_acc.append(gender_train_acc[0])\n",
    "        gender_train_1_acc.append(gender_train_acc[1])\n",
    "        gender_test_0_acc.append(gender_test_acc[0])\n",
    "        gender_test_1_acc.append(gender_test_acc[1])\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"model\"] = models\n",
    "    result_df[\"F1_train\"] = f1_train_list\n",
    "    result_df[\"Acc_train\"] = acc_train_list\n",
    "    result_df[\"Acc_train_female\"] = gender_train_0_acc\n",
    "    result_df[\"Acc_train_male\"] = gender_train_1_acc\n",
    "    result_df[\"F1_val\"] = f1_test_list\n",
    "    result_df[\"Acc_val\"] = acc_test_list\n",
    "    result_df[\"Acc_val_female\"] = gender_test_0_acc\n",
    "    result_df[\"Acc_val_male\"] = gender_test_1_acc\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_gender(y_true, y_pred, gender_labels):\n",
    "    y_true = np.array(y_true)\n",
    "    gender_0_ind = np.where(np.array(gender_labels) == 0)\n",
    "    gender_1_ind = np.where(np.array(gender_labels) == 1)\n",
    "    #print(gender_labels)\n",
    "    #print(\"overall acc:\", accuracy(y_pred, y_true))\n",
    "    ##sns.heatmap(confusion_matrix(y_pred, y_true), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "    \n",
    "    gender_0_accuracy = accuracy(y_pred[gender_0_ind], y_true[gender_0_ind])\n",
    "    #print(\"Gender 0 accuracy:\", gender_0_accuracy)\n",
    "    #sns.heatmap(confusion_matrix(y_pred[gender_0_ind], y_true[gender_0_ind]), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "               \n",
    "    gender_1_accuracy = accuracy(y_pred[gender_1_ind], y_true[gender_1_ind])\n",
    "    #print(\"Gender 1 accuracy:\", gender_1_accuracy)\n",
    "    #sns.heatmap(confusion_matrix(y_pred[gender_1_ind], y_true[gender_1_ind]), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "    \n",
    "    return gender_0_accuracy, gender_1_accuracy\n",
    "    \n",
    "    \n",
    "#trained_model, f1_train, f1_test, acc_train, acc_test = fit_evaluate_model(models[0], X_train, X_test_final, y_train, y_test_final, print_evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model(model, X_train, X_test, y_train, y_test, train_gender, validation_gender, model_name=None, print_evaluation=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"Training set\")\n",
    "    y_pred = model.predict(X_train)\n",
    "    f1_train = f1_score(y_train, y_pred)\n",
    "    acc_train = accuracy(y_train, y_pred)\n",
    "    #print(\"F1:\", f1_train)\n",
    "    #print(\"Acc:\", acc_train)\n",
    "    \n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_train,y_pred))\n",
    "        cm = confusion_matrix(y_train,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "        \n",
    "    #print(len(y_train))\n",
    "    #print(len(train_gender))\n",
    "    gender_train_acc = evaluate_by_gender(y_train, y_pred, train_gender)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #print(\"Testing set\")\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "    acc_test = accuracy(y_test, y_pred)\n",
    "    #print(\"F1:\", f1_test)\n",
    "    #print(\"Acc:\", acc_test)\n",
    "\n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "        \n",
    "    gender_test_acc = evaluate_by_gender(y_test, y_pred, validation_gender)\n",
    "        \n",
    "    \n",
    "    return model, f1_train, f1_test, acc_train, acc_test, gender_train_acc, gender_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model_old(model, X_train, X_test, y_train, y_test, model_name=None, print_evaluation=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"Training set\")\n",
    "    y_pred = model.predict(X_train)\n",
    "    f1_train = f1_score(y_train, y_pred)\n",
    "    acc_train = accuracy(y_train, y_pred)\n",
    "    #print(\"F1:\", f1_train)\n",
    "    #print(\"Acc:\", acc_train)\n",
    "    \n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_train,y_pred))\n",
    "        cm = confusion_matrix(y_train,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #print(\"Testing set\")\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "    acc_test = accuracy(y_test, y_pred)\n",
    "    #print(\"F1:\", f1_test)\n",
    "    #print(\"Acc:\", acc_test)\n",
    "\n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    return model, f1_train, f1_test, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# obtained after hyperparameter tunning\n",
    "models = [\n",
    "    SVC(kernel='linear', C=0.005),  \n",
    "    LinearSVC(penalty=\"l1\", dual=False),\n",
    "    DecisionTreeClassifier(max_depth=3,criterion=\"gini\"),\n",
    "    RandomForestClassifier(n_estimators=5,max_depth=3,min_samples_leaf=2,min_samples_split=4, random_state=2), \n",
    "    LogisticRegression(C=0.7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def run_feature_selection(selector_class, model=models[3], estimator=None):\n",
    "    f1s = []\n",
    "    f1s_train = []\n",
    "    best_chosen = []\n",
    "    f1_max = 0.0\n",
    "    f1_train_ = None\n",
    "    \n",
    "    for i in range(1, X_train.shape[1]):\n",
    "\n",
    "        if estimator == None:\n",
    "            selector = selector_class(model, n_features_to_select=i, step=1)\n",
    "        else:\n",
    "            selector = selector_class(estimator, n_features_to_select=i, step=1)\n",
    "        selector = selector.fit(X_train, y_train)\n",
    "\n",
    "        chosen = selector.get_support()\n",
    "        X_train_selected = X_train[:, chosen]\n",
    "        X_test_selected = X_test[:, chosen]\n",
    "        #print(X_train_selected.shape)\n",
    "\n",
    "        trained_model = fit_evaluate_model_old(model, X_train_selected, X_test_selected, y_train, y_test, model_name=None, print_evaluation=False)\n",
    "\n",
    "        #print()\n",
    "\n",
    "        y_pred = model.predict(X_train_selected)\n",
    "        f1_train = f1_score(y_train, y_pred)\n",
    "        #print(f1_train)\n",
    "        f1s_train.append(f1_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        #print(f1)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "        if f1 > f1_max:\n",
    "            f1_max = f1\n",
    "            f1_train_ = f1_train\n",
    "            best_chosen = chosen\n",
    "            \n",
    "        if f1_train > 0.95 and f1_train - f1 > 0.4:\n",
    "            print(\"Overfitting..\")\n",
    "            break\n",
    "        \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 10\n",
    "    fig_size[1] = 5\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "    plt.plot(range(1,len(f1s)+1), f1s, label=\"Validation set\")\n",
    "    plt.plot(range(1,len(f1s)+1), f1s_train, label=\"Training set\")\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    print(\"--------------\")\n",
    "    indexes = list(range(X_train.shape[1]))\n",
    "    chosen_indexes = np.array(indexes)[best_chosen]\n",
    "    print(chosen_indexes)\n",
    "    print(len(chosen_indexes))\n",
    "    print(\"Training:\", f1_train_)\n",
    "    print(\"Testing:\", f1_max)\n",
    "    \n",
    "    return chosen_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [50, 100, 250, 500, 1000, 2000]:\n",
    "    print(\"m =\", m)\n",
    "    df = pd.read_csv(\"motifs_{}_m{}.csv\".format(band, m))\n",
    "    grouped_df = df.groupby([\"labels\", \"genders\"]).apply(sort_data)\n",
    "    \n",
    "    training_set, testing_set, final_testing_set = generate_datasets(grouped_df)\n",
    "    target = \"label\"\n",
    "    y_train = training_set[target]\n",
    "    X_train = np.array(training_set.drop(target, axis=1))\n",
    "    X_test = np.array(testing_set.drop(target, axis=1))\n",
    "    y_test = testing_set[target]\n",
    "\n",
    "    X_test_final = np.array(final_testing_set.drop(target, axis=1))\n",
    "    y_test_final = final_testing_set[target]\n",
    "    \n",
    "    result_df = run_evaluation_result(X_train, X_test,  y_train, y_test, train_gender, validation_gender)\n",
    "    display(result_df)\n",
    "\n",
    "    features = run_feature_selection(RFE)\n",
    "    X_train = X_train[:, features]\n",
    "    X_test = X_test[:, features]\n",
    "    X_test_final = X_test_final[:, features]\n",
    "    selected_columns = training_set.columns[features]\n",
    "    print(sorted(selected_columns))\n",
    "    \n",
    "    print(\"Train+validation, test\")\n",
    "    result_df = run_evaluation_result(np.concatenate((X_train, X_test), axis=0), X_test_final, np.concatenate((y_train,y_test)), y_test_final, np.concatenate((train_gender,validation_gender)), test_final_gender)\n",
    "    display(result_df)\n",
    "    \n",
    "    print(\"===============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
