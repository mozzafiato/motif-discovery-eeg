{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains several models on a given dataset and prints/saves the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_gender(y_true, y_pred, gender_labels):\n",
    "    y_true = np.array(y_true)\n",
    "    gender_0_ind = np.where(np.array(gender_labels) == 0)\n",
    "    gender_1_ind = np.where(np.array(gender_labels) == 1)\n",
    "    #print(gender_labels)\n",
    "    #print(\"overall acc:\", accuracy(y_pred, y_true))\n",
    "    ##sns.heatmap(confusion_matrix(y_pred, y_true), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "    \n",
    "    gender_0_accuracy = accuracy(y_pred[gender_0_ind], y_true[gender_0_ind])\n",
    "    #print(\"Gender 0 accuracy:\", gender_0_accuracy)\n",
    "    #sns.heatmap(confusion_matrix(y_pred[gender_0_ind], y_true[gender_0_ind]), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "               \n",
    "    gender_1_accuracy = accuracy(y_pred[gender_1_ind], y_true[gender_1_ind])\n",
    "    #print(\"Gender 1 accuracy:\", gender_1_accuracy)\n",
    "    #sns.heatmap(confusion_matrix(y_pred[gender_1_ind], y_true[gender_1_ind]), annot=True, fmt='d')\n",
    "    #plt.show()\n",
    "    \n",
    "    return gender_0_accuracy, gender_1_accuracy\n",
    "    \n",
    "    \n",
    "#trained_model, f1_train, f1_test, acc_train, acc_test = fit_evaluate_model(models[0], X_train, X_test_final, y_train, y_test_final, print_evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model(model, X_train, X_test, y_train, y_test, train_gender, validation_gender, model_name=None, print_evaluation=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"Training set\")\n",
    "    y_pred = model.predict(X_train)\n",
    "    f1_train = f1_score(y_train, y_pred)\n",
    "    acc_train = accuracy(y_train, y_pred)\n",
    "    #print(\"F1:\", f1_train)\n",
    "    #print(\"Acc:\", acc_train)\n",
    "    \n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_train,y_pred))\n",
    "        cm = confusion_matrix(y_train,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "        \n",
    "    #print(len(y_train))\n",
    "    #print(len(train_gender))\n",
    "    gender_train_acc = evaluate_by_gender(y_train, y_pred, train_gender)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #print(\"Testing set\")\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "    acc_test = accuracy(y_test, y_pred)\n",
    "    ##print(\"F1:\", f1_test)\n",
    "    #print(\"Acc:\", acc_test)\n",
    "\n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "        \n",
    "    gender_test_acc = evaluate_by_gender(y_test, y_pred, validation_gender)\n",
    "        \n",
    "    \n",
    "    return model, f1_train, f1_test, acc_train, acc_test, gender_train_acc, gender_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model_old(model, X_train, X_test, y_train, y_test, model_name=None, print_evaluation=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Training set\")\n",
    "    y_pred = model.predict(X_train)\n",
    "    f1_train = f1_score(y_train, y_pred)\n",
    "    acc_train = accuracy(y_train, y_pred)\n",
    "    print(\"F1:\", f1_train)\n",
    "    print(\"Acc:\", acc_train)\n",
    "    \n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_train,y_pred))\n",
    "        cm = confusion_matrix(y_train,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Testing set\")\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "    acc_test = accuracy(y_test, y_pred)\n",
    "    print(\"F1:\", f1_test)\n",
    "    print(\"Acc:\", acc_test)\n",
    "\n",
    "    if print_evaluation:\n",
    "\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    return model, f1_train, f1_test, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, _X, _y, _cv=5):\n",
    "      '''Function to perform 5 Folds Cross-Validation\n",
    "       Parameters\n",
    "       ----------\n",
    "      model: Python Class, default=None\n",
    "              This is the machine learning algorithm to be used for training.\n",
    "      _X: array\n",
    "           This is the matrix of features.\n",
    "      _y: array\n",
    "           This is the target variable.\n",
    "      _cv: int, default=5\n",
    "          Determines the number of folds for cross-validation.\n",
    "       Returns\n",
    "       -------\n",
    "       The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
    "       'recall', 'f1' for both training set and validation set.\n",
    "      '''\n",
    "      _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "      results = cross_validate(estimator=model,\n",
    "                               X=_X,\n",
    "                               y=_y,\n",
    "                               cv=_cv,\n",
    "                               scoring=_scoring,\n",
    "                               return_train_score=True)\n",
    "      \n",
    "      return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "              \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "              \"Training F1 scores\": results['train_f1'],\n",
    "              \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "              \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "              \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "              \"Validation F1 scores\": results['test_f1'],\n",
    "              \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_model(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    trial=Trials()\n",
    "    \n",
    "    if type(model) == LogisticRegression:\n",
    "        best = optimize_lr(trial, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return LogisticRegression(C=best['C'])\n",
    "        \n",
    "    elif type(model) == DecisionTreeClassifier:\n",
    "        best = optimize_dtree(trial, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return DecisionTreeClassifier(\n",
    "            max_depth=int(best['max_depth']),\n",
    "            criterion=int(best['criterion'])\n",
    "        )\n",
    "        \n",
    "    elif type(model) == RandomForestClassifier:\n",
    "        best = optimize_rf(trial, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=int(best['n_estimators']),\n",
    "            max_depth=int(best['max_depth']),\n",
    "            min_samples_leaf=int(best['min_samples_leaf']),\n",
    "            min_samples_split=int(best['min_samples_split']))   \n",
    "    \n",
    "            \n",
    "    elif type(model) == SVC:\n",
    "        best = optimize_svc(trial, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return SVC(C=best['C'], kernel=best['kernel'])\n",
    "    \n",
    "    elif type(model) == LinearSVC:\n",
    "        best = optimize_linearSvc(trial, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return LinearSVC(penalty=best['penalty'])\n",
    "    \n",
    "    else:\n",
    "        print(\"Model has no parameters to tune.\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(models, X_train, X_test, y_train, y_test, hyperparam_tunning=False):\n",
    "    \n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        \n",
    "        if hyperparam_tunning:\n",
    "            model = get_tuned_model(model, X_train, X_test, y_train, y_test)\n",
    "            print(model)\n",
    "            \n",
    "            print(\"Cross validation from training set:\")\n",
    "            results = cross_validation(model, X_train, y_train, _cv=5)\n",
    "            for key, value in results.items():\n",
    "                print(key, value)\n",
    "\n",
    "            print(\"\")\n",
    "            trained_model, _, _, _, _ = fit_evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        if type(model) == RandomForestClassifier:\n",
    "            explainer = shap.TreeExplainer(trained_model)\n",
    "            shap_values = explainer.shap_values(X_train)\n",
    "            shap.summary_plot(shap_values, X_train)\n",
    "        \n",
    "        print(\"*****************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    SVC(), \n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(), \n",
    "    LogisticRegression()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe,hp,Trials\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "\n",
    "def objective_rf(params):\n",
    "    est=int(params['n_estimators'])\n",
    "    md=int(params['max_depth'])\n",
    "    msl=int(params['min_samples_leaf'])\n",
    "    mss=int(params['min_samples_split'])\n",
    "    X_train = params['X_train']\n",
    "    X_test = params['X_test']\n",
    "    y_train = params['y_train']\n",
    "    y_test = params['y_test']\n",
    "    \n",
    "    model=RandomForestClassifier(n_estimators=est,max_depth=md,min_samples_leaf=msl,min_samples_split=mss)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    score=f1_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "def optimize_rf(trial, X_train, X_test, y_train, y_test):\n",
    "    params={\n",
    "        'n_estimators':hp.choice('n_estimators', [5, 10, 15, 20]),\n",
    "        'max_depth':hp.choice('max_depth',[3, 4, 5, 10, 20]),\n",
    "        'min_samples_leaf':hp.choice('min_samples_leaf', [1, 2, 3, 4]),\n",
    "        'min_samples_split':hp.choice('min_samples_split', [1, 2, 3, 4]),\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    best=fmin(\n",
    "        fn=objective_rf,\n",
    "        space=params,\n",
    "        algo=tpe.suggest,trials=trial,\n",
    "        max_evals=100,\n",
    "        rstate=np.random.default_rng(seed)\n",
    "    )\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree \n",
    "\n",
    "def objective_dtree(params):\n",
    "    md=int(params['max_depth'])\n",
    "    criterion=params['criterion']\n",
    "    X_train = params['X_train']\n",
    "    X_test = params['X_test']\n",
    "    y_train = params['y_train']\n",
    "    y_test = params['y_test']\n",
    "    \n",
    "    model=DecisionTreeClassifier(max_depth=md,criterion=criterion)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    score=f1_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "def optimize_dtree(trial, X_train, X_test, y_train, y_test):\n",
    "    params={\n",
    "        'max_depth':hp.choice('max_depth', [3, 4, 5, 10, 20]),\n",
    "        'criterion': hp.choice('criterion', [\"gini\", \"log_loss\"]),\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    best=fmin(\n",
    "        fn=objective_dtree,\n",
    "        space=params,\n",
    "        algo=tpe.suggest,trials=trial,\n",
    "        max_evals=100,\n",
    "        rstate=np.random.default_rng(seed)\n",
    "    )\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "def objective_svc(params):\n",
    "    \n",
    "    c=params['C']\n",
    "    kernel=params['kernel']\n",
    "    X_train = params['X_train']\n",
    "    X_test = params['X_test']\n",
    "    y_train = params['y_train']\n",
    "    y_test = params['y_test']\n",
    "    \n",
    "    if kernel=='rbf':\n",
    "        model = SVC(kernel='rbf')\n",
    "    else:\n",
    "        model=SVC(kernel='linear', C=c)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    score=f1_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "def optimize_svc(trial, X_train, X_test, y_train, y_test):\n",
    "    params={\n",
    "        'C': hp.choice('C', [0.0005, 0.001, 0.01, 0.1, 0.5]),\n",
    "        'kernel': hp.choice('linear', 'rbf'),\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "    }\n",
    "    \n",
    "    best=fmin(\n",
    "        fn=objective_svc,\n",
    "        space=params,\n",
    "        algo=tpe.suggest,trials=trial,\n",
    "        max_evals=100,\n",
    "        rstate=np.random.default_rng(seed)\n",
    "    )\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "def objective_linearSvc(params):\n",
    "    \n",
    "    penalty=params['penalty']\n",
    "    X_train = params['X_train']\n",
    "    X_test = params['X_test']\n",
    "    y_train = params['y_train']\n",
    "    y_test = params['y_test']\n",
    "    \n",
    "    if penalty==\"l1\":\n",
    "        model=LinearSVC(penalty=penalty, dual=False)\n",
    "    else:\n",
    "        model=LinearSVC(penalty=\"l2\")\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    score=f1_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "def optimize_linearSvc(trial, X_train, X_test, y_train, y_test):\n",
    "    params={\n",
    "        'penalty': hp.choice('penalty', [\"l1\", \"l2\"]),\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "    }\n",
    "    \n",
    "    best=fmin(\n",
    "        fn=objective_linearSvc,\n",
    "        space=params,\n",
    "        algo=tpe.suggest,trials=trial,\n",
    "        max_evals=100,\n",
    "        rstate=np.random.default_rng(seed)\n",
    "    )\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "def objective_lr(params):\n",
    "    \n",
    "    c=params['C']\n",
    "    X_train = params['X_train']\n",
    "    X_test = params['X_test']\n",
    "    y_train = params['y_train']\n",
    "    y_test = params['y_test']\n",
    "    \n",
    "    model=LogisticRegression(C=c)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    score=f1_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "def optimize_lr(trial, X_train, X_test, y_train, y_test):\n",
    "    params={\n",
    "        'C': hp.choice('C', [0.1, 0.5, 0.7, 1]),\n",
    "        'penalty': hp.choice('penalty',['l1', 'l2',]),\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "    }\n",
    "    \n",
    "    best=fmin(\n",
    "        fn=objective_lr,\n",
    "        space=params,\n",
    "        algo=tpe.suggest,trials=trial,\n",
    "        max_evals=100,\n",
    "        rstate=np.random.default_rng(seed)\n",
    "    )\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"motifs_all_electrodes_beta_m70_train_balanced.csv\")\n",
    "validation_set = pd.read_csv(\"motifs_all_electrodes_beta_m70_val_balanced.csv\")\n",
    "\n",
    "target = \"label\"\n",
    "y_train = training_set[target]\n",
    "X_train = np.array(training_set.drop(target, axis=1))\n",
    "X_test = np.array(validation_set.drop(target, axis=1))\n",
    "y_test = validation_set[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_models(models, X, y, hyperparam_tunning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_evaluation_result(X_input_train, X_input_test,  y_input_train, y_input_test, train_input_gender, validation_input_gender):\n",
    "    f1_train_list = []\n",
    "    f1_test_list = []\n",
    "    acc_train_list = []\n",
    "    acc_test_list = []\n",
    "    gender_train_0_acc = []\n",
    "    gender_train_1_acc = []\n",
    "    gender_test_0_acc = []\n",
    "    gender_test_1_acc = []\n",
    "\n",
    "    for model in models:\n",
    "        #print(model)\n",
    "        trained_model, f1_train, f1_test, acc_train, acc_test, gender_train_acc, gender_test_acc = fit_evaluate_model(model, X_input_train, X_input_test, y_input_train, y_input_test, train_input_gender, validation_input_gender, print_evaluation=False)\n",
    "        f1_train_list.append(f1_train)\n",
    "        f1_test_list.append(f1_test)\n",
    "        acc_train_list.append(acc_train)\n",
    "        acc_test_list.append(acc_test)\n",
    "        gender_train_0_acc.append(gender_train_acc[0])\n",
    "        gender_train_1_acc.append(gender_train_acc[1])\n",
    "        gender_test_0_acc.append(gender_test_acc[0])\n",
    "        gender_test_1_acc.append(gender_test_acc[1])\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"model\"] = models\n",
    "    result_df[\"F1_train\"] = f1_train_list\n",
    "    result_df[\"Acc_train\"] = acc_train_list\n",
    "    result_df[\"Acc_train_female\"] = gender_train_0_acc\n",
    "    result_df[\"Acc_train_male\"] = gender_train_1_acc\n",
    "    result_df[\"F1_val\"] = f1_test_list\n",
    "    result_df[\"Acc_val\"] = acc_test_list\n",
    "    result_df[\"Acc_val_female\"] = gender_test_0_acc\n",
    "    result_df[\"Acc_val_male\"] = gender_test_1_acc\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
